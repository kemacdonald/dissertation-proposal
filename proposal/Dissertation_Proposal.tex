\documentclass[]{elsarticle} %review=doublespace preprint=single 5p=2 column
%%% Begin My package additions %%%%%%%%%%%%%%%%%%%
\usepackage[hyphens]{url}
\usepackage{lineno} % add
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\bibliographystyle{elsarticle-harv}
\biboptions{sort&compress} % For natbib
\usepackage{graphicx}
\usepackage{booktabs} % book-quality tables
%% Redefines the elsarticle footer
%\makeatletter
%\def\ps@pprintTitle{%
% \let\@oddhead\@empty
% \let\@evenhead\@empty
% \def\@oddfoot{\it \hfill\today}%
% \let\@evenfoot\@oddfoot}
%\makeatother

% A modified page layout
\textwidth 6.75in
\oddsidemargin -0.15in
\evensidemargin -0.15in
\textheight 9in
\topmargin -0.5in
%%%%%%%%%%%%%%%% end my additions to header

\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \usepackage{fontspec}
  \ifxetex
    \usepackage{xltxtra,xunicode}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{â‚¬}
\fi
% use microtype if available
\IfFileExists{microtype.sty}{\usepackage{microtype}}{}
\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \usepackage[unicode=true]{hyperref}
\fi
\hypersetup{breaklinks=true,
            bookmarks=true,
            pdfauthor={},
            pdftitle={How social contexts shape information gathering, attention, and memory during familiar language comprehension and novel word learning},
            colorlinks=true,
            urlcolor=blue,
            linkcolor=magenta,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{0}
% Pandoc toggle for numbering sections (defaults to be off)
\setcounter{secnumdepth}{0}
% Pandoc header


\usepackage[nomarkers]{endfloat}

\begin{document}
\begin{frontmatter}

  \title{How social contexts shape information gathering, attention, and memory
during familiar language comprehension and novel word learning}
    \author[Stanford University]{Kyle MacDonald\corref{c1}}
   \ead{kylem4@stanford.edu} 
   \cortext[c1]{Corresponding Author}
    
  \begin{abstract}
  Language comprehension in grounded contexts involves integrating visual
  and linguistic information through decisions about visual fixation. But
  when the visual signal also contains information about the language
  source -- as in the case of written text or sign language -- how do we
  decide where to look? Here, we hypothesize that eye movements during
  language comprehension represent an adaptive response. Using two case
  studies, we show that, compared to English-learners, young signers
  delayed their gaze shifts away from a language source, were more
  accurate with these shifts, and produced a smaller proportion of
  nonlanguage-driven shifts (E1). Next, we present a well-controlled,
  confirmatory experiment, showing that English-speaking adults produced
  fewer nonlanguage-driven shifts when processing printed text compared to
  spoken language (E2). Together, these data suggest that people adapt to
  the value of seeking different information in order to increase the
  chance of rapid and accurate language understanding.
  \end{abstract}
  
 \end{frontmatter}

\section{Overview of the proposed
work}\label{overview-of-the-proposed-work}

The study of eye movements during language comprehension has provided
fundamental insights into the interaction between conceptual
representations of the world and the incoming linguistic signal. For
example, research shows that adults and children will rapidly shift
visual attention upon hearing the name of an object in the visual scene,
with a high proportion of shifts occurring prior to the offset of the
word (Allopenna, Magnuson, and Tanenhaus 1998; Tanenhaus et al. 1995).
Moreover, researchers have found that conceptual representations
activated by fixations to the visual world can modulate subsequent eye
movements during language processing (Altmann and Kamide 2007).

The majority of this work has used eye movements as a measure of the
output of the underlying language comprehension process, often using
linguistic stimuli that come from a disembodied voice. But in real world
contexts, people also gather information about the linguistic signal by
fixating on the language source. Consider a speaker who asks you to
``Pass the salt'' but you are in a noisy room, making it difficult to
understand the request. Here, comprehension can be facilitated by
gathering information via (a) fixations to the nonlinguistic visual
world (i.e., encoding the objects that are present in the scene) or (b)
fixations to the speaker (i.e., reading lips or perhaps the direction of
gaze).

But, this situation creates a tradeoff where the listener must decide
what kind of information to gather and at what time. How do we decide
where to look? We propose that people modulate their eye movements
during language comprehension in response to tradeoffs in the value of
gathering different kinds of information. We test this adaptive tradeoff
account using two case studies that manipulate the value of different
fixation locations for language understanding: a) a comparison of
processing sign vs.~spoken language in children (E1), and b) a
comparison of processing printed text vs.~spoken language in adults
(E2). Our key prediction is that competition for visual attention will
make gaze shifts away from the language source less valuable than
fixating the source of the linguistic signal, leading people to generate
fewer exploratory, nonlanguage-driven eye movements.

\section{Background literature}\label{background-literature}

\section{Completed work}\label{completed-work}

\subsection{Social cues modulate attention and memory in
cross-situational word
learning}\label{social-cues-modulate-attention-and-memory-in-cross-situational-word-learning}

\subsection{Eye movements in American Sign Language comprehension
reflect a tradeoff between gathering information about the linguistic
signal and the nonlinguistic visual
world}\label{eye-movements-in-american-sign-language-comprehension-reflect-a-tradeoff-between-gathering-information-about-the-linguistic-signal-and-the-nonlinguistic-visual-world}

\subsection{Children learning spoken language leverage visual
information from social partners to facilitate language
comprehension}\label{children-learning-spoken-language-leverage-visual-information-from-social-partners-to-facilitate-language-comprehension}

\section{Proposed work}\label{proposed-work}

\subsection{Degraded linguistic signal and visual
information}\label{degraded-linguistic-signal-and-visual-information}

\subsection{Eye movements for information seeking during novel word
learning}\label{eye-movements-for-information-seeking-during-novel-word-learning}

\subsection{Active selection of referents during social
cross-situational word
learning}\label{active-selection-of-referents-during-social-cross-situational-word-learning}

\section{Conclusion}\label{conclusion}

\newpage 

\section{References}\label{references}

\setlength{\parindent}{-0.3in} \setlength{\leftskip}{0.2in} \noindent

\hypertarget{refs}{}
\hypertarget{ref-allopenna1998tracking}{}
Allopenna, Paul D, James S Magnuson, and Michael K Tanenhaus. 1998.
``Tracking the Time Course of Spoken Word Recognition Using Eye
Movements: Evidence for Continuous Mapping Models.'' \emph{Journal of
Memory and Language} 38 (4). Elsevier: 419--39.

\hypertarget{ref-altmann2007real}{}
Altmann, Gerry, and Yuki Kamide. 2007. ``The Real-Time Mediation of
Visual Attention by Language and World Knowledge: Linking Anticipatory
(and Other) Eye Movements to Linguistic Processing.'' \emph{Journal of
Memory and Language} 57 (4). Elsevier: 502--18.

\hypertarget{ref-tanenhaus1995integration}{}
Tanenhaus, Michael K, Michael J Spivey-Knowlton, Kathleen M Eberhard,
and Julie C Sedivy. 1995. ``Integration of Visual and Linguistic
Information in Spoken Language Comprehension.'' \emph{Science} 268
(5217). The American Association for the Advancement of Science: 1632.

\end{document}


