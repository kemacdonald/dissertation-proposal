---
title: Balancing looks to people and to objects during novel word learning
bibliography: dp_library.bib
output: pdf_document
author: Kyle MacDonald
header-includes:
  - \setlength{\parindent}{2em}
  - \setlength{\parskip}{1em}
  - \linespread{1.25}
---

```{r load_packages, include = FALSE}
knitr::opts_chunk$set(echo=F, warning=F, cache=T, message=F, sanitize=T,
                      out.width = "95%", fig.path='figs/',
                      fig.align = "center", fig.pos = "tb")
library(here)
library(knitr); library(xtable); library(png); library(grid)
library(tidyverse)
```

# Dissertation overview {#background}

Learning a first language should be hard. Consider that even concrete nouns are often used in complex contexts with multiple possible referents, which in turn have many conceptually natural properties that a speaker could talk about. This ambiguity creates the potential for an (in principle) unlimited amount of referential uncertainty in the learning task.^[This problem is a simplified version of Quine's \textit{indeterminacy of reference} [@quine19600]: That there are many possible meanings for a word ("Gavigai") that include the referent ("Rabbit") in their extension, e.g., "white," "rabbit," "dinner." Quine's broader philosophical point was that different meanings ("rabbit" and "undetached rabbit parts") could actually be extensionally identical and thus impossible to tease apart.]. Moreover, to find meaning in language requires rapdily establishing reference during real-time interaction where the incoming information is dynamic, multimodal, and transient. Remarkably, children's word learning proceeds despite limitations in computational resources challenges, with estimates of adult vocabularies ranging between 50,000 to 100,000 distinct lexical concepts [@bloom2002children]. How do learners infer and retain such a large variety of word meanings from data with this kind of ambiguity?

Statistical learning theories offer a solution to this problem by aggregating cross-situational statistics across labeling events to identify underlying word meanings [@yu2007rapid; @siskind1996computational]. Recent experimental work has shown that both adults and young infants can use word-object co-occurrence statistics to learn words from individually ambiguous naming events [@smith2008infants; @vouloumanos2008fine]. For example, @smith2008infants taught 12-month-olds three novel words simply by repeating consistent novel word-object pairings across 10 ambiguous exposure trials. Moreover, computational models suggest that cross-situational learning can scale up to learn adult-sized lexicons, even under conditions of considerable referential uncertainty [@smith2011cross].

While all cross-situational learning models agree that the input is the co-occurrence between words and objects and the output is stable word-object mappings, they disagree about several key points. First, alternative models propose different underlying representaions that support long-term retention of word-object labels. One approach has been to model learning as a process of updating connection strengths between multiple word-object links with the underlying representation being a distributed word-object co-occurrence matrix [@mcmurray2012word]. Another approach argues that learners just store a single word-object hypothesis, only switching to a new hypothesized word-object link when there is sufficient negative evidence [@trueswell2013propose]. 

Researchers also disagree about how to best characterize the ambiguity of the input to cross-situational learning mechanisms. One way to quantify the uncertainty in a naming event is to show adults video clips of caregiver-child interactions and measure their accuracy at guessing the meaning of an intended referent (Human Simulation Paradigm: HSP [Gillette, Gleitman, Gleitman, and Lederer, 1999]). Using the HSP, @medina2011words found that approximately 90% of learning episodes were ambiguous (< 33% accuracy) and only 7% were relatively unambiguous (> 50% accuracy). In contrast, @yurovsky2013statistical found a higher proportion of clear naming events, with approximately 30% being unambiguous (> 90% accuracy). Moreover, @cartmill2013quality showed that the proportion of unambiguous naming episodes varies across different parent-child dyads, with some parents rarely providing highly informative contexts and others’ doing so more often.

Thus, representations in cross-situational word learning can appear distributional or discrete, and the input to statistical learning mechanisms can vary along a continuum from low to high ambiguity. Taken together, these alternatives highlight the need for a better understanding of how behaviors  in-the-moment language comprehension processes shape the long-term retention of word-object labels.  

TODO: 

  * say more here -- this point is crucial to the need for my dissertation work. need to bring in the idea of how decision-making within a social learning context can shape learning and subsequent decisions.
  * Also need to make the point that information-theoretic accounts (tested in the case study of familiar language comprehension) have something to say about learners' behavior during the labeling moment.


Moreover, researchers have only begun to understand these processes within spoken language learning for a narrow range of learning contexts. For example, the majority of prior work on language comprehension and cross-situational learning has used linguistic stimuli that come from a disembodied voice and a visual world that consists of concrete objects. As a result, we know much less about how learning unfolds in contexts that include a communicative partner who can modulate the ambiguity of the input to cross-situational learning mechanisms. 

This gap is important to address since social-pragmatic theories of language acquisition have long emphasized the importance of the social context for first language acquisition [@bloom2002children; @clark2009first; @hollich2000breaking]. Moroever, experimental work has shown that even children as young as 16 months prefer to map novel words to objects that are the target of a speaker’s gaze and not their own [@baldwin1993infants]. In an analysis of naturalistic parent-child labeling events, @yu2012embodied found that young learners tended to retain labels that were accompanied by clear referential cues, which served to make a single object dominant in the visual field. And correlational studies have demonstrated links between early intention-reading skills (e.g., gaze following) and later vocabulary growth [@brooks2005development; @carpenter1998social].

A second open question for models of early language acquisition is how learners adapt their behaviors to seek information that supports their word learning. For example, we know little about how eye movements adapt to contexts where fixating on another person supports language acquisition as in the case of children learning a signed language. This developmental context creates a tradeoff where young signers must decide whether to look at their social partner to gather information about language or to look at the nonlinguistic visual world to gather information about objects. This channel competition potentially complicaties the link between the in-the-moment processes of establishing reference and long-term rentention of object labels.

My dissertation work directly explores how familiar language comprehension and novel word learning adapts to a wider variety of learning contexts, including sign language, language accompanied by social cues to reference, and language produced in noisy auditory contexts. To do this, I argue that it is useful to deconstruct the concrete word learning task into three parts (see @frank2016performance or @mcmurray2012word) and ask how each sub-component adapts to different contexts:

  1. comprehending familiar words [in-the-moment]
  2. following a social cue to reference [in-the-moment]
  3. retaining a novel word-object link [across multiple moments]

\noindent 
In our previous work, we characterized how children and adults choose to allocate visual attention between a social partner and objects during familiar American Sign Language (ASL) comprehension ([Chapter 1](#ch1)). We then compared the dynamics of eye movements during familiar ASL processing to those of spoken language learners, showing that ASL-learners gather more information about the linguistic signal before shifting away from a language source compared to spoken language learners. We proposed an information-seeking account to explain these modality-based differences and tested predictions of our account across a variety of language comprehension contexts. We found the same pattern of eye movements for English-speaking adults processing displays of printed text and for both children and adults processing speech in noisy auditory environments ([Chapter 2](ch2)). These results suggest that listeners flexibly adapt eye movements to the value of seeking higher value visual information to support their goal of rapid langauge comprehension.

In a separate line of work, we have asked how the presence of a social cue to reference -- a speaker's gaze -- could change the representations that support novel word learning ([Chapter 3](#ch3)). Our results suggest that word learneers stored representations with different levels of fidelity depending on the amount of ambiguity present during learning. In the absence of a referential cue to word meaning, learners tended to store more alternative word-object links. In contrast, when gaze was present learners stored less information, showing behavior consistent with tracking a single hypothesis. Thus, word learners flexibly respond to the amount of ambiguity in the input, and as referential uncertainty increases, they tend to store more information.  

Here, I propose a study that will connect our prior work on eye movements for information seeking during familiar language comprehension with our work on the effects of social cues on cross-situational word learning. The goal of this study is to test how predictions of our information seeking account generalize to the novel word learning context. Testing these preditions will increase our understanding of how listeners flexibly adapt the dynamics of eye movements to seek higher value information to support word learning. Overall, these results will synthesize ideas from research on social-pragmatic theories of language acqusition and work on goal-based vision to increase our knowledge of how in-the-moment decisions about how to allocate visual attention affect word learning over time.

# Completed work {#prior}

## Chapter 1: Dividing visual attention to language and objects during real-time American Sign Language comprehension {#ch1}

```{r sol_stimuli, eval = F}
readPNG(here("prior_work/figs_png/sol_fig1_trial_info.png")) %>% grid.raster()
```

When children interpret spoken language in real time, linguistic information drives rapid shifts in visual attention to objects in the visual world, which can provide insights into the development of efficiency in lexical access. But how does language influence visual attention when the linguistic signal and the visual world are both processed via the visual channel? We developed precise measures of eye movements during real-time comprehension of a visual-manual language, American Sign Language (ASL), by 29 native, monolingual ASL-learning children (16-53 mos, 16 deaf, 13 hearing) and 16 fluent deaf adult signers. All signers showed evidence of rapid, incremental language comprehension, initiating eye movements prior to sign offset.  Deaf and hearing ASL-learners showed remarkably similar gaze patterns, suggesting that the in-the-moment dynamics of eye movements during ASL processing are shaped by the constraints of processing a visual language in real time and not by differential access to auditory information in day-to-day life. Finally, variation in children’s ASL processing was positively correlated with age and vocabulary size. Thus, despite channel competition, allocation of visual attention during ASL comprehension reflects information processing skills that are fundamental for language acquisition regardless of language modality.

```{r sol_plot}
readPNG("../prior_work/figs_png/sol_fig2_timecourse_all.png") %>% grid.raster()
```

## Chapter 2: An information seeking account of differences in eye movements between spoken and signed language during real-time language processing {#ch2}

The study of eye movements during language comprehension has provided fundamental insights into the interaction between conceptual representations of the world and the incoming linguistic signal. For example, research shows that adults and children will rapidly shift visual attention upon hearing the name of an object in the visual scene, with a high proportion of shifts occurring prior to the offset of the word [@allopenna1998tracking; @tanenhaus1995integration]. Moreover, researchers have found that conceptual representations activated by fixations to the visual world can modulate subsequent eye movements during language processing [@altmann2007real]. 

The majority of this work has used eye movements as a measure of the output of the underlying language comprehension process, often using linguistic stimuli that come from a disembodied voice. But in real world contexts, people also gather information about the linguistic signal by fixating on the language source. Consider a speaker who asks you to "Pass the salt" but you are in a noisy room, making it difficult to understand the request. Here, comprehension can be facilitated by gathering information via (a) fixations to the nonlinguistic visual world (i.e., encoding the objects that are present in the scene) or (b) fixations to the speaker (i.e., reading lips or perhaps the direction of gaze). But, this situation creates a tradeoff where the listener must decide what kind of information to gather and at what time. How do we decide where to look? We propose that people modulate their eye movements during language comprehension in response to tradeoffs in the value of gathering different kinds of information. 

We test this adaptive tradeoff account using two case studies that manipulate the value of different fixation locations for language understanding: a) a comparison of processing sign vs. spoken language in children (E1), and b) a comparison of processing printed text vs. spoken language in adults (E2). Our key prediction is that competition for visual attention will make gaze shifts away from the language source less valuable than fixating the source of the linguistic signal, leading people to generate fewer exploratory, nonlanguage-driven eye movements.

```{r speed_acc_trio_plot}
readPNG(here("/prior_work/figs_png/speed_acc_trio.png")) %>% grid.raster()
```

Some of our work has explored how the presence of another person changes the set of information seeking behaviors available [@macdonald2017info]. Inspired by theories of natural vision that characterize eye movements as an information seeking mechanism, we asked whether children and adults would allocate more visual attention to a speaker when the linguistic signal was noisy to support the goal of rapid language understanding. We used an eye-tracking task to measure participants' gaze patterns while they processed clear or degraded speech (speech with brown noise added). Both children and adults spent more time fixating on the speaker in the degraded speech context. Interestingly, children and adults were also more accurate in word recognition even though the speech was noisy and difficult to process. This result suggests that listeners were compensating for the uncertainty in the auditory channel by gathering visual information from the speaker. Critically, listeners would not have been able to gather this information if the speaker was not present (e.g., listening to a noisy recording) and in clear view.

```{r speed_acc_noise_plot, eval = F}
readPNG(here("/prior_work/figs_png/speed_acc_noise.png")) %>% grid.raster()
```

## Chapter 3: Social cues to reference modulate attention and memory during cross-situaitonal word learning {#ch3}

Our prior work provides evidence that the social context can modulate the content of the learner's hypothesis space [@macdonald2017social]. Inspired by ideas from Social-pragmatic theories of language acquisition that emphasize the importance of social cues for word learning [@clark2009first; @hollich2000breaking; @bloom2002children], we showed adults a series of word learning contexts that varied in ambiguity depending on whether there was a useful social cue to reference available (a speaker's gaze). We then measured learners' memory for alternative word-object links. People flexibly responded to the amount of ambiguity in the input, and as uncertainty increased, they tended to store more word-object hypotheses. Moreover, we found that learners stored representations with different levels of fidelity as a function of the reliability of the social cue. When the speaker was a less reliable source of information, learners distributed attention and memory broadly, storing more hypotheses.

These results provide evidence that the content of learners' hypothesis spaces changed as a function of social information. Further suppport for this idea comes from experimental work showing that even children as young as 16 months prefer to map novel words to objects that are the target of a speaker’s gaze and not their own [@baldwin1993infants], and analyses of naturalistic parent-child labeling events shows that young learners tended to retain labels accompanied by clear referential cues, which served to make a single object dominant in the visual field [@yu2012embodied]. One important direction for future research is to measure the full causal pathway from variation in social information through children's hypothesis spaces to their information seeking behaviors. For example, it would be interesting to know whether learners' subsequent questions or decisions about where to allocate attention would be affected by the social context in which they were first exposed to a new word.

```{r soc_xsit_plot, out.width="85%"}
readPNG("../prior_work/figs_png/soc_xsit.png") %>% grid.raster()
```

# Proposed work {#proposal}

The goal of the proposed work is to understand how children use the presence of social information to help solve the problem of mapping concrete nouns to their referents amidst referential uncertainty. We will test an information-theoretic account of eye movements within a context where the child has uncertainty over word-object links. Our hypothesis is that gathering visual information from a speaker becomes more useful when uncertainty over word meanings is high and the goal is to learn word-object links. As the learner builds stronger word-object links via repeated exposures to co-occurence information, we predict that the value of allocating fixations to the speaker should decrease while the value of looking to the objects should increase. 

## Balancing looks to people and to objects during word learning

This project aims to answer the following research questions:

* How does access to social cues shape in-the-moment decisions about visual fixation? 
* How do children balance looks to people and to objects over the course of learning a new concept? 
* Do children use prior knowledge to select fixation behaviors that best support word learning? 

The word learning context is an interesting case because learners are working towards multiple goals: comprehending speech in the moment (a dynamic intergration of linguistic and visual signal with prior knowledge) and figuring out what the new word refers to in the visual scene. Thus, eye movements during concept formation can be used to gather visual information from the speaker (e.g., eye gaze or mouth movements) or about the nonlinguistic visual world (encoding objects). How do we explain where children look as they acquire more information in-the-moment of language comprehension and as they build a learning history about the correct word-object mapping?  This question can be formalized as a sequential decision making problem where children make fixation choice based on (1) their knowledge of the target concept, (2) the value of fixating a speaker for linguistic processing, and (3) the cost of each eye movement. 

Framing fixation behaviors as a goal-based decision-making problem allows us to connect to formal models of action selection developed to explain 

A growing body of psychological research has used the OED framework as a metaphor for active learning. The idea is that when people make decisions, they engage in a similar process of evaluating the "usefulness" of different actions relative to their learning goals. And they select behaviors that maximize the potential for gaining information. A success of the OED account is that it can capture a wide range of information seeking behaviors, including verbal question asking [@ruggeri2015children], planning interventions in causal learning tasks [@cook2011science], and decisions about where to look during scene understanding [@najemnik2005optimal]. Figures 1 and 2 present schematic overviews of how OED principles could shape the learning process for two of these domains -- causal learning (Figure 1) and word learning (Figure 2).


One compelling use case of OED metaphor as a model of human behavior comes from @nelson2005finding study of eye movements during novel concept learning. Their model combined Bayesian probabilistic learning, which represents current knowledge as a probability distribution over concepts, with an OED model that calculated the usefulness of different patterns of eye movements. Here, eye movements were modeled as a type of question-asking behavior that gathered visual information about the target concept. @nelson2005finding found that participants' eye movements aligned with predictions from the OED model. Specifically, participants changed the dynamics of eye movements depending on how well they learned the target concepts. Early in learning, when the concepts were unfamiliar, the model generated a broader, less efficient distribution of fixations to explore all candidate features that could be used to categorize the stimulus. However, after the model began to learn the target concepts, eye movement patterns shifted to become more efficient and focused on a single stimulus dimension to maximize accuracy. This shift from exploratory to efficient eye movements matched adult performance on the task, suggesting that people's behavior was sensible given the structure of the learning problem and the uncertainty in the context.

The intuition is that people balance fixating a speaker and fixating objects to support concept learning. The question is whether models of Bayesian concept learning and Optimal Experiment Design (Neslon & Cottrell, 2007) provide a good explanation of children’s eye movements. How far can we get using a purely computational information seeking decision model? 

### Pilot

When gaze cued adults’ visual attention, they showed stronger memory for the word-object link compared to when a gaze cue was absent (Figure 2). This result suggest that social information does more than modulate how people allocate their visual attention (more than a filter); instead, social cues change the strength of the inference.

```{r gaze_xsit_plot}
readPNG("../prior_work/figs_png/gaze_xsit.png") %>% grid.raster()
```

### Design

### Predictions

The prediction is that the dynamics of eye movement will shift over the course of learning. In the beginning of the task, learners will distribute fixations to prioritize gathering information about the objects or about disambiguating reference (e.g., gathering a gaze cue). After learning the word-object links, people will shift and start to distribute more fixations to the speaker to gather visual information that supports comprehension of the speech, showing the behavioral signatures measured in the familiar language comprehension task.

\newpage
# References 

\setlength{\parindent}{-0.3in} 
\setlength{\leftskip}{0.3in}
\noindent
