---
title: "Integrating social and statisical information during language comprehension and word learning"
subtitle: Dissertation proposal
author: 'Kyle MacDonald'
date: "Committee: Virginia Marchman, Hyo Gweon, Jay McClleland, & Michael C. Frank"
bibliography: dp_library.bib
output: 
  bookdown::pdf_document2:
    toc: true
header-includes:
  - \setlength{\parindent}{2em}
  - \setlength{\parskip}{1em}
  - \linespread{1.25}
---

```{r set-options, include = FALSE}
knitr::opts_chunk$set(echo=F, warning=F, cache=T, message=F, sanitize=T,
                      out.width = "95%", fig.path='figs/',
                      fig.align = "center", fig.pos = "tb")
```

```{r load-packages}
library(here)
library(knitr); library(xtable); library(png); library(grid)
library(tidyverse)
fig_read_path <- "prior_work/figs_png/"
```

\newpage

# Background {#background}

Learning a new word should be hard. Consider that even concrete nouns are often produced in complex contexts with multiple possible referents, which in turn have many conceptually natural properties that a speaker could talk about. This ambiguity creates the potential for an (in principle) unlimited amount of referential uncertainty in the learning task ^[This problem is a simplified version of Quine's \textit{indeterminacy of reference} [@quine19600]: That there are many possible meanings for a word ("Gavigai") that include the referent ("Rabbit") in their extension, e.g., "white," "rabbit," "dinner." Quine's broader philosophical point was that different meanings ("rabbit" and "undetached rabbit parts") could be extensionally identical and thus impossible to tease apart.]. Moreover, to find meaning in language requires rapidly establishing reference during real-time interaction where the incoming information is dynamic, multimodal, and transient. Remarkably, language comprehension and word learning proceed despite these challenges, with estimates of adult vocabularies ranging from 50,000 to 100,000 distinct lexical concepts [@bloom2002children]. How do learners infer and retain such a large variety of word meanings from data with this kind of ambiguity?

Statistical learning theories offer a solution by aggregating cross-situational statistics across labeling events to identify underlying word meanings [@yu2007rapid; @siskind1996computational]. Experimental work has shown that both adults and young infants can use word-object co-occurrence statistics to learn words from individually ambiguous naming events [@smith2008infants; @vouloumanos2008fine]. For example, @smith2008infants taught 12-month-olds three novel words simply by repeating consistent novel word-object pairings across ten ambiguous exposure trials. Moreover, computational models suggest that cross-situational learning can scale to learn adult-sized lexicons, even under conditions of considerable referential uncertainty [@smith2011cross].

While all models of cross-situational learning agree that the input is co-occurrences between words and objects and the output is stable word-object mappings, they disagree about several key points. First, alternative models propose different underlying representations that support long-term retention of word-object labels. One approach characterizes learning as a process of updating connection strengths between multiple word-object links with the underlying representation being a distributed word-object co-occurrence matrix [@mcmurray2012word]. Another approach argues that learners store a single word-object hypothesis, only switching to a new hypothesized link when there is sufficient negative evidence [@trueswell2013propose]. 

In addition to the debate about representation, researchers disagree about the amount of ambiguity in the input. Some studies have found that a majority of naming events are ambiguous [@medina2011words], while other work has found a higher proportion of clear naming events [@yurovsky2013statistical]. Moreover, @cartmill2013quality showed that the proportion of unambiguous naming episodes varies across different parent-child dyads, with some parents rarely providing highly informative input and others’ doing so more often. The critical point is that variability in referential uncertainty varies across naming events and should be included in models of cross-situational word learning. 

Thus, cross-situational learning can appear distributional or discrete, and the input to statistical learning mechanisms can vary along a continuum from low to high ambiguity depending on features of the communicative context. The fact that social partners can modulate referential ambiguity highlights a gap in the experimental work on cross-situational word learning. That is, the majority of this research has used linguistic stimuli generated by a disembodied voice coupled with a visual world that consists of pictures of concrete objects. In contrast, labeling events outside the lab often occur during face-to-face communicative interactions, which provide the learner with a rich set of visual cues (e.g., gestures, facial expressions, and mouth movements) that can modulate information features of the input to cross-situational learning mechanisms. 

This gap is important since social-pragmatic theories of language acquisition have long emphasized the role of social contexts in supporting first language acquisition [@bloom2002children; @clark2009first; @hollich2000breaking]. Moreover, experimental work has shown that even children as young as 16 months prefer to map novel words to objects that are the target of a speaker’s gaze and not their own [@baldwin1993infants] and that analyses of naturalistic parent-child labeling events show that young learners tended to retain labels that were accompanied by clear referential cues, which served to make a single object dominant in the visual field [@yu2012embodied]. Finally, correlational studies have demonstrated links between early intention-reading skills (e.g., gaze following) and later vocabulary growth [@brooks2005development; @carpenter1998social].

A second open question is how the processes that support cross-situational word learning adapt to the wide variety of learning contexts that children experience. That is, most of the prior empirical work has focused on word learning in spoken language and within contexts where children have clear access to the auditory and visual information. This assumption, however, does not capture the variability in the type of input to cross-situational learning mechanisms. For example, we know relatively little about how children's actions might adapt to contexts where fixating on another person is critical for language acquisition as in the case of children learning a visual-manual language, like American Sign Language (ASL). The sign learning context creates an interesting tradeoff where children must decide whether to look at their social partner to gather information about language or to look at the nonlinguistic visual world to gather information about objects. This channel competition might potentially complicate the link between the in-the-moment process of establishing reference and the long-term retention of object labels.

My dissertation work takes a step towards addressing these open questions. I have asked how behaviors that support familiar language comprehension and cross-situational word learning adapt to a wider variety of contexts, including (a) language accompanied by social cues to reference, (b) processing a visual-manual language like ASL, and (c) comprehending language in noisy auditory environments. These contexts represent a diverse sample of language environments that share a key feature: demands of the context change the value of gathering and storing certain kinds of information for the goal of language comprehension and learning. 

In the rest of this proposal, I briefly review my completed dissertation work before outlining the proposed experiment. At a high-level, the new study aims to connect my prior research on eye movements for information seeking with my work on cross-situational word learning in the presence of social cues to reference. We will ask how children shift the dynamics of their eye movements to gather different kinds of visual information as they experience multiple exposures to word-object links. This study aims to synthesize ideas from the social-pragmatic and statistical accounts of language acquisition with goal-based theories of vision to increase our understanding of how children's real-time information selection supports their longer-term learning.

# Completed work {#prior}

## Eye movements during real-time American Sign Language comprehension {#ch1}

```{r sol-stimuli, out.width="80%", fig.cap = "Configuration of visual stimuli (1A) and trial structure (1B) for one question type (sentence final wh-phrase) shown in the central video in the real-time ASL processing task."}
readPNG(here(fig_read_path, "sol_fig1_trial_info.png")) %>% grid.raster()
```

### Study overview

```{r sol-plot, out.width="90%", fig.cap = "The time course of looking behavior for ASL-proficient adults (A) and young ASL- learners (C). The curves show mean proportion looking to the signer (dark grey), the target image (black), and the distracter image (light grey). The grey shaded region marks the analysis window (600-2500 ms); error bars represent 95\\% CI computed by non-parametric bootstrap. The mean proportion of each target sign length processed before shifting visual attention away from the language source to a named object for adults (B) and children (D). The diamond indicates the mean estimate for all signs. The dashed vertical line corresponds to a median proportion of 1.0. A median of greater than 1.0 reflects response latencies that occur before the offset of the target sign; a median of less than or equal to 1.0 reflects response latencies that occur after target sign offset. Error bars represent 95\\% Highest Density Intervals."}

readPNG(here::here(fig_read_path, "sol_fig2_timecourse_all.png")) %>% grid.raster()
```

When children interpret spoken language in real time, linguistic information drives rapid shifts in visual attention to objects in the world, which can provide insights into the processes underlying real-time language comprehension [@allopenna1998tracking]. But how does language influence visual attention when the linguistic signal and the visual world are both processed via the visual channel? In this work, we measured eye movements during real-time comprehension of a visual-manual language, American Sign Language (ASL), by 29 native ASL-learning children (16-53 mos, 16 deaf, 13 hearing) and 16 fluent deaf adult signers. 

### Methods

Participants viewed the task on a 27" monitor. On each trial, pictures of two familiar objects appeared on the screen, a target object corresponding to the target noun, and a distracter object (see Fig. \@ref(fig:sol-stimuli)). All picture pairs were matched for visual salience based on prior studies with spoken language (Fernald et al., 2008). Between the two pictures was a central video of an adult female signing the name of one of the pictures. Participants saw 32 test trials with five filler trials (e.g., “YOU LIKE PICTURES? MORE WANT?”) interspersed to maintain children’s interest. 

Participants’ gaze patterns were video recorded and later coded frame-by-frame at 33-ms resolution by coders blind to target side. We computed two measures of ASL processing. First shift reaction time (RT), was the latency to shift from the central signer to the target picture on all signer-to-target shifts, measured from the target-noun onset. And Accuracy was the mean proportion of time spent looking at the target picture out of the total time looking at either target or distracter picture over the 600 to 2500 ms window from target noun onset. 

### Key findings

All signers showed evidence of rapid, incremental language comprehension, tending to initiate an eye movement before sign offset (see Fig. \@ref(fig:sol-plot)). Moreover, Deaf and hearing ASL-learners showed similar gaze patterns, suggesting that the in-the-moment dynamics of eye movements during ASL processing are shaped by the constraints of processing a visual language in real time and not by differential access to auditory information in day-to-day life. Finally, variation in children’s ASL processing was positively correlated with age and vocabulary size. These results show that, despite competition for attention within a single modality, both children and adults rapidly shifted visual attention away from a social partner and towards objects before sign offset. This result suggests that there is a robust link between processing an object label and quickly allocating visual attention to that object in the visual world. 

In addition to this parallel result, we did find differences in the timing of young signers' gaze shifts as compared to children learning spoken language. That is, when we analyzed RTs, we saw that signers' first shift were substantially slower than spoken language learners' RTs in a series of unpublished datasets using a parallel real-time processing task (Fernald & Marchman). This RT difference held even though the English-learners were processing the same words and were similar in age. The next set of studies in my dissertation explore the causal factors that could account for the changes in dynamics of eye movements across spoken and signed languages.

## Eye movements during spoken and signed language processing: An information seeking account {#ch2}

### Study overview

Language comprehension in grounded, social contexts involves extracting meaning from the linguistic signal and mapping it to the surrounding world. But how should listeners prioritize integrating different kinds of visual information? In this work, we proposed that listeners flexibly adapt their gaze behaviors in response to features of the social context, seeking visual information from their social partners when it was useful for language comprehension. We present evidence for our account using three case studies sampled from a diverse set of language processing contexts: sign language processing, processing of dynamic displays of printed text, and processing spoken language within noisy auditory environments.

### Methods

The design and procedure of all three studies were nearly identical to the work on children's eye movements in American Sign Language reviewed in section \@ref(ch1). In study 1, we compared the timing and accuracy of eye movements for children learning ASL to children learning a spoken language. We used parallel real-time language comprehension tasks where participants processed familiar sentences (e.g., "Where's the ball?") while looking at a simplified visual world with three fixation targets (a center stimulus that varied by condition, a target picture, and a distracter picture). In study 2, hearing adults processed dynamic displays of printed text. We chose text processing because, like sign language, the majority of information relevant for comprehension is located at a single location. In study 3, we compared eye movements of both adults and children processing spoken language in clean or noisy auditory environments.

Across all three studies, we analyzed the timing and accuracy of initial gaze shifts after the onset of the target noun. The timescale of the analysis is milliseconds and focuses on a single decision within a series of decisions about where to look during sentence processing. We made this choice because first shifts provide a window onto changes in the underlying dynamics of how listeners integrate linguistic information with the decision processes that generate eye movements.

### Key findings 

First, compared to children learning spoken English (n=80) and adults (n=25), young ASL-learners (n=30) and adults (n= 16) delayed their gaze shifts away from a language source, were more accurate with these shifts, and produced a smaller proportion of random shifting behavior (see Fig. \@ref(fig:speed-acc-trio-plot)). Next, English-speaking adults produced fewer random gaze shifts when processing dynamic displays of printed text compared to processing spoken language. Finally, 3-5 year-olds (n=39) and adults (n=31) delayed the timing of gaze shifts away from a speaker's face when processing speech in a noisy environment, which resulted in fewer random eye movements, and more accurate gaze shifts, despite the noisier processing context (see Fig. \@ref(fig:speed-acc-noise-plot)). These results provide evidence that young listeners, like adults, will adapt their gaze patterns to the demands of different processing environments by seeking out visual information from social partners to support language comprehension.

```{r speed-acc-stim, out.width="60%", fig.cap = "Stimuli for the experiment comparing eye movements in signed and spoken language.  Panel A shows the layout of the fixation locations for all tasks: the center stimulus, the target, and the distracter. Panel B shows the five center stimulus items: a static geometric shape (Bullseye), a static image of a familiar object (Object), a person speaking (Face), a person signing (ASL), and printed text (Text)."}

readPNG(here::here(fig_read_path, "trio_stimuli.png")) %>% grid::grid.raster()
```

```{r speed-acc-trio-plot, fig.cap = "Timecourse looking, first shift Reaction Time (RT), and Accuracy results. Panel A shows the overall looking to the center, target, and distracter stimulus for each context. Panel B shows the distribution of RTs for each participant. Each point represents a participant's average RT. Color represents the processing context. Panel C shows the same information but for first shift accuracy. Signers were slower but more accurate with their shifts."}
readPNG(here::here(fig_read_path, "speed_acc_trio.png")) %>% grid.raster()
```

```{r speed-acc-noise-plot, out.width="95%", fig.cap = "Timecourse looking, first shift Reaction Time (RT), and Accuracy results. Panel A shows the overall looking to the center, target, and distracter stimulus for each processing condition and age group. Panel B shows the distribution of RTs for each participant and the pairwise contrast between the noise and clear conditions. Panel C shows the same information but for first shift accuracy."}
readPNG(here::here(fig_read_path, "speed_acc_noise.png")) %>% grid.raster()
```

### Link to the current proposal

Our work on eye movements during familiar language comprehension suggests that the dynamics of gaze adapt to very different processing contexts to achieve the goal of rapid language understanding. These results raise an interesting question: do learners show similar adaptation and flexibility during novel word learning? In the next section, I describe a line of work where we investigated how the presence of referential cues in the social context (e.g., a speaker's eye gaze) changes the ambiguity of the input to statistical word learning, which, in turn, modulates the amount of information that learners stored from a labeling event.


## Social cues modulate attention and memory during cross-situational word learning {#ch3}

```{r soc-xsit-stimuli, out.width="60%", fig.cap = "Exposure and test trials from Experiments 1-4. The top left panel shows an exposure trial in the No-gaze condition using the schematic gaze cue (Experiment 1). The top right panel shows an exposure trial in the Gaze condition using the video gaze cue (Experiments 2-4). Participants saw either Gaze or No-gaze exposure trials depending on condition assignment, and participants saw both types of test trials: Same (bottom left panel) and Switch (bottom right panel). On Same trials, the object that participants chose during exposure appeared with a new novel object. On Switch trials the object that participants did not choose appeared with a new novel object." }

readPNG(here::here(fig_read_path, "soc_xsit_stimuli_new.png")) %>% grid.raster()
```

### Study overview

```{r soc-xsit-plot, out.width="75%", fig.cap = "Panel A shows accuracy on Same and Switch test trials as a function of the interval between exposure and test trials. Panel B shows memory performance as a function of the reliability of the speaker's prior gaze cues. Error bars indicate 95\\% confidence intervals computed by non-parametric bootstrap."}

readPNG(here::here(fig_read_path, "soc_xsit.png")) %>% grid.raster()
```

Because children hear language in environments that contain many things to talk about, learning the meaning of even the simplest word requires making inferences under uncertainty. A cross-situational statistical learner can aggregate across naming events to form stable word-referent mappings, but this approach neglects an important source of information that can reduce referential uncertainty: social cues from speakers (e.g., eye gaze). In four large-scale experiments with adults, we tested the effects of varying referential uncertainty in cross-situational word learning using social cues. 

### Method

Experiments 1-4 followed a similar design and procedure. We posted a set of Human Intelligence Tasks (HITs) to Amazon Mechanical Turk. Adults saw a total of 16 trials: eight exposure trials and eight test trials. On each trial, they heard one novel word, saw a set of novel objects, and were asked to guess which object went with the word (see Fig. \@ref(fig:soc-xsit-stimuli)). Before seeing exposure and test trials, participants completed four practice trials with familiar words and objects. These trials familiarized participants to the task and allowed us to exclude participants who were unlikely to perform the task as directed, either because of inattention or because their computer audio was turned off.

After the practice trials, participants were told that they would now hear novel words and see novel objects and that their task was to select the referent that “goes with each word.” Over the course of the experiment, participants heard eight novel words two times, with one exposure trial and one test trial for each word. Four of the test trials were Same trials in which the object that participants selected on the exposure trial was shown with a set of new novel objects. The other four test trials were Switch trials in which one of the objects was chosen at random from the set of objects that the participant did not select on exposure.

In Experiment 3, we modified the cross-situational learning paradigm to include a block of 16 familiarization trials (8 exposure trials and 8 test trials) at the beginning of the experiment. These trials served to establish the reliability of the speaker’s gaze. To manipulate reliability, we varied the proportion of Same/Switch trials that occurred during the familiarization block. Recall that on Switch trials the gaze target did not show up at test, which provided evidence that the speaker’s gaze was not a reliable cue to reference. Reliability was a between-subjects manipulation such that participants either saw 8, 6, 4, 2, or 0 Switch trials during familiarization, which created the 0%, 25%, 50%, 75%, and 100% reliability conditions. After the familiarization block, participants completed another block of 16 trials (8 exposure trials and 8 test trials).

### Key findings

Social cues shifted learners away from tracking multiple hypotheses and towards storing only a single hypothesis (Experiments 1 and 2; Panel A of Fig. \@ref(fig:soc-xsit-plot)). Also, learners were sensitive to graded changes in the strength of a social cue, and when it became less reliable, they were more likely to store multiple hypotheses (Experiment 3; Panel B of Fig. \@ref(fig:soc-xsit-plot)). Finally, learners stored fewer word-referent mappings in the presence of a social cue even when given the opportunity to visually inspect the objects for the same amount of time (Experiment 4). These results suggest that the representations underlying cross-situational word learning of concrete object labels are quite flexible: In conditions of greater uncertainty, learners stored a broader range of information.

# Proposed work {#proposal}

## Overview 

The goal of the proposed study is to understand the features that modulate children's decisions about visual fixation within grounded, social word learning contexts. The word learning context is particularly interesting because children are working towards multiple goals -- i.e., figuring out what someone else is referring to (disambiguation of reference in the moment) and learning what a new word means (building up a concept over time). This creates a scenario where eye movements could be used to gather visual information to achieve these different goals. For example, looking to the speaker can be used to facilitate real-time comprehension (audiovisual language perception) and to disambiguate reference (reading the direction of gaze or pointing). On the other hand, looking to the nonlinguistic visual world can be useful for the long-term retention of word meanings by facilitating stronger word-object representations.

In short, looks to speakers and objects are both useful. So how do children decide where to allocate their limited visual attention as they are learning new words? In this study, we will measure the dynamics of children's eye movements during word learning and ask how these dynamics change as a function of repeated exposure to word-object mappings. We will directly manipulate the value of seeking visual information from different fixation locations (speakers vs. objects) by including learning episodes where speakers provide visual cues to disambiguate reference. We hypothesize that fixations to a speaker are most useful early on in learning, when uncertainty over word meanings is high and when a social gaze cue is present. After multiple exposures to novel word-object pairings, we predict that the value of allocating fixations to the speaker should decrease while the value of looking at the objects should increase. This change in value should be reflected in the timing of learners' gaze shifts away from a social partner and to the objects in the visual world.

## Theoretical contributions

This work will address several important questions for theories of word learning and language acquisition more broadly. First, how do statistical learning mechanisms operate over fundamentally social input? This study brings together social information that reduces referential ambiguity in the learning moment and statistical information that reduces ambiguity over time. Moreover, the majority of prior work on statistical word learning has used linguistic stimuli that come from a disembodied voice, removing a rich set of multimodal cues (e.g., gestures, facial expressions, mouth movements) that occur during face-to-face communication. By including a social partner as fixation target, this work will add to our understanding how social contexts shape the input to word learning mechanisms. 

Second, how do children use visual information to support their language learning? In this work, we characterize eye movements as information-seeking and frame the learner's task as decision making under time constraints. By using this theoretical framework, we can leverage analytic tools developed over the past decades for studying the dynamics of rapid choice: Drift Diffusion Models [@ratcliff2015individual]. The benefit of this modeling approach is that it provides a tool for quantifying differences in underlying psychological constructs of interest -- speed of processing vs. cautious response strategies -- that might lead to measurable behavioral differences. At a higher level, this work will integrate top-down, goal-based models of vision [@hayhoe2005eye] with work on language-driven eye movements [@allopenna1998tracking].

Finally, this study will increase our understanding of how children's in-the-moment behaviors, such as decisions about visual fixation, connect to learning that unfolds over longer timescales. Following @mcmurray2012word, we separate situation-time behaviors (figuring out the referent of a word) from developmental-time processes (slowly forming mappings between words and concepts). Moreover, by studying changes in patterns of eye movements over the course of learning, we will add to a recent body of empirical work that emphasizes the importance of linking real-time information selection to longer-term statistical learning [@yu2011you]. 

## Pilot

```{r gaze-xsit-plot, out.width = "85%", fig.cap = "Panel A shows adults' allocation of visual attention during exposure trials. Panel B shows adults' stronger memory for novel word-object links when there was a gaze cue present, even when they had fixated on the target objects for similar amounts of time during learning."}

readPNG(here::here(fig_read_path, "gaze_xsit.png")) %>% grid.raster()
```

In our prior work (discussed in \@ref(ch3)), we found that the presence of a gaze cue shifted adults away from storing multiple word-object links and towards tracking a single hypothesis. However, those experiments relied on an offline measurement of word learning (a button press on test trials) and an indirect measure of attention during learning (self-paced decisions about how long to inspect the visual scene). To address these limitations, in a pilot study we adapted the social cross-situational learning paradigm to use eye-tracking methods. Moving to an eye-tracking procedure allowed us to answer two questions: 

1. How does the presence of gaze alter the distribution of visual attention during labeling? 
2. Does the presence of a gaze cue change the strength of learners' inferences about word-object links? 

### Method

We tracked adults' (n=30) eye movements while they watched a series of ambiguous word-learning events (16 novel words) organized into pairs of exposure and test trials (32 trials total). All trials consisted of a set of two novel objects and one novel word. Participants were randomly assigned to either the Gaze condition in which a speaker looked at one of the objects on exposure trials or the No-Gaze condition in which a speaker looked straight on exposure trials. Every exposure trial was followed by a test trial, where participants heard the same novel word paired with a new set of two novel objects. One of the objects in the set had appeared in the exposure trial ("kept" object), while the other object had not previously appeared in the experiment ("novel" object). 

### Key findings

We found that the presence of social cues focused adults' visual attention on a single object (Panel A of Fig. \@ref(fig:gaze-xsit-plot)) during labeling and that gaze-cued attention leads to stronger inferences for word-referent pairings. That is, adults allocated more attention to the correct object at test in the gaze condition despite fixating on the target objects for similar amounts of time during learning (Panel B of Fig. \@ref(fig:gaze-xsit-plot)). This result suggests that social cues do more than modulate how people allocate their visual attention; instead, social cues may change the strength of the underlying inferences that support word learning.

### Limitations

There were several key limitations of our pilot study. First, we chose to start the liguistic stimulus  as soon as the images and the speaker appeared on the screen (i.e., at trial onset). This made it difficult to analyze the timing and accuracy of first shifts decisions away from the speaker and to the objects. Second, this trial structure did not allow us to measure decisions about visual fixation that occur before the start of language comprehension while learners are first gathering information about the visual world. Finally, the linguistic stimuli consisted of sixteen pseudowords recorded by a speech synthesizer and presented in isolation, thus removing any sentential context. Presenting isolated words is unlikely to work with younger age groups and does not allow us to separate decisions about fixations made during language processing more broadly from decisions that occur after the onset of the target noun -- a critical distinction for our modeling of the underlying decision-making process. 

## Study proposal

This study will address the limitations of our pilot work by making the stimuli, design, and procedure parallel to our work on familiar language processing. We will use the analytic techniques developed for analyzing first gaze shifts to better understand how children's decisions about visual fixation during object labeling change as a function of learning a new word in the presence of social cues to reference. 

\noindent
The study aims to answer the following specific research questions:

1. How does access to social cues to reference from a speaker change decisions about where to look during object labeling? 
2. How do decisions about where to allocate visual attention change over the course of learning a new word? 
3. What is the relationship between looking behavior during object labeling and memory of new words?

### Participants 

We will collect data from adults (n=30) using the Stanford Psychology Credit Pool. And we will recruit data from children ages 3-5 years (n=50) from the Bing Nursery school. These sample sizes are comparable to those used in our prior eye tracking work.  

### Stimuli and Design

```{r speed-acc-novel-stimuli, out.width = "75%", fig.cap = "Stimuli for the proposed study, including trial structure, fixation targets in the visual world, and gaze cue manipulation."}
readPNG(here::here(fig_read_path, "speed_acc_novel_stim.png")) %>% grid.raster()
```

We will compare the timing and accuracy of eye movements during a real-time cross-situational learning task where participants process sentences that contain a novel word (e.g., "Where's the *dax*?") while looking at a simplified visual world with 3 fixation targets (a video of a speaker and two images of unfamiliar objects). The trial structure will be identical to our work on eye movements during familiar language comprehension reviewed in section \@ref(ch2) (see Fig. \@ref(fig:speed-acc-novel-stimuli)). This parallel structure will allow us to take advantage of the techniques for analyzing changes in the dynamics of initial gaze shifts after the onset of the target noun. 

Participants will watch a series of ambiguous word-learning events organized into pairs of exposure and test trials. All trials will consist of a set of two unfamiliar objects and one novel word. Each exposure trial will be followed by a test trial where participants will hear the same novel word but paired with a new set of two novel objects. One of the objects in the set will have appeared in the exposure trial (target object), while the other object will be new, i.e., not previously shown in the experiment (distractor object). Critically, participants will see four exposure trials for each of four novel word-object pairings over the course of the experiment. By including multiple exposures, we can analyze changes in decisions about visual fixation as participants build up stronger word-object links. There will be four novel words, two trial types, and four exposures to each word-object pairing, for a total of 32 trials (5-7 minutes). This experiment length is similar to our previous work on familiar language processing and should work with our target age range.

We will also manipulate the value of the speaker as a target for visual fixation by varying whether the speaker provides a post-nominal gaze cue. Participants will be randomly assigned to either a Gaze condition or No-Gaze condition. In the Gaze condition, the speaker will look at one of the objects after producing the novel label, thus fully disambiguating reference. In the No-Gaze condition, the speaker will continue to look straight-ahead at the participant throughout the exposure trial. This will be a within-participants manipulation with adults (blocked design) and a between-participants manipulation with children. 

### Analysis plan

We will follow the analysis plan developed in our prior work on familiar language comprehension and described in section \@ref(ch2). First, we will analyze first shift accuracy and reaction time (RT). RT corresponds to the latency to shift away from the central stimulus to either picture measured from the onset of the target noun. Accuracy corresponds to whether participants' first gaze shift landed on the target or the distracter picture.

Next, we will use two model-based analyses to link observable behavior (accuracy and RT) to underlying psychological constructs. We will use an exponentially weighted moving average (EWMA) method [@vandekerckhove2007fitting] to classify gaze shifts as language-driven or random. In contrast to the standard RT/accuracy analysis, the EMWA approach allows us to quantify participants' willingness to generate gaze shifts after noun onset but before collecting sufficient information to seek the named referent. Concretely, the EWMA models change in random shifting behavior as a function of RT. For each RT, the model generates two values: a "control statistic" (CS, which captures the running average accuracy of first shifts) and an "upper control limit" (UCL, which captures the pre-defined limit of when accuracy would be categorized as above chance level). Here, the CS is an expectation of random shifting to either the target or the distracter image (nonlanguage-driven shifts), or a Bernoulli process with probability of success 0.5. As RTs get slower, we assume that participants have gathered more information and should become more accurate (language-driven), or a Bernoulli process with probability success > 0.5. Using this model, we can quantify the proportion of gaze shifts that were language-driven as opposed to random responding. 

Finally, we will use drift-diffusion models (DDMs) [@ratcliff2015individual] to ask whether any measured behavioral differences in accuracy and RT are driven by a shift towards a more cautious responding strategy (i.e., gathering more visual information) or by more efficient information processing of the linguistic stimuli. We will follow @vandekerckhove2007fitting, and select shifts categorized as language-driven by the EWMA and fit a hierarchical Bayesian drift-diffusion model (HDDM). The DDM quantifies differences in the underlying decision process that lead to different patterns of behavior. The model assumes that people accumulate noisy evidence in favor of one alternative with a response generated when the evidence crosses a pre-defined decision threshold. Here, we focus on two parameters of interest: *boundary separation*, which indexes the amount of evidence gathered before generating a response (higher values indicate more cautious responding) and *drift rate*, which indexes the amount of evidence accumulated per unit time (higher values indicate more efficient processing). 

As exploratory analyses, we will also analyze two measures of participants' gaze patterns that are on the timescale of seconds and take into account a series of decisions about where to look over the course of two time windows:  (1) trial onset -- start of the linguistic stimulus and (2) target noun onset -- end of the trial. First, we will compute proportion looking to each fixation target (objects vs. the speaker). Second, we will compute the entropy of eye movements [@yu2011you]. To calculate entropy, we define the total number of fixations in a time window as $L$ and assume that each fixation $f_m$ lasts a certain amount of time $T(f_m)$ and normalize each fixation time by the overall looking time within the trial. Thus, the entropy of eye movements during a time window $t$ will be:

$$E(t) = -\sum_{m=1}^{L} \frac{T(f_m)}{\sum(f_m)} log \frac{T(f_m)}{\sum (f_m)}$$
\noindent
Higher entropy values reflect more fixations overall (i.e., more gaze shifts between targets) and a more even distribution of looking across gaze targets. Lower entropy reflects a pattern of long fixations with some short fixations. The intuition is that more rapid attention switches within exposure trials and more even looking times will indicate participants' uncertainty about word-object pairings.

For all statistical models, we will use the `rstanarm` [@gabry2016rstanarm] package to fit Bayesian mixed-effects regressions. The mixed-effects approach will allow us to model any nested structures in our data -- i.e., multiple trials for each participant and item, and a within-participants manipulation of gaze cue.

```{r speed-acc-nov-preds, out.width = "95%", fig.pos="h", fig.cap = "Predictions for learning and test trials for different measures and analyses."}

readPNG(here::here("proposal/preds.png")) %>% grid.raster()
```

### Predictions

We have two key behavioral predictions (see the table below for more detailed predictions). First, the distribution of attention to speakers compared to objects will shift over the course of learning. Early in the task, participants will allocate fixations that prioritize gathering visual information about the objects or about disambiguating reference. After experiencing multiple exposures to a word-object pairing, learners will start to distribute fixations such that they support the goal of rapid comprehension of the incoming speech, showing behavioral signatures of eye movements for familiar language processing that we found in our prior work. 

Second, the presence of a gaze cue will change the dynamics of children's decisions about visual fixation. We hypothesize that a post-nominal gaze cue will increase the information value of fixating on the speaker. This manipulation should cause participants to allocate more fixations to the speaker when gaze is present, leading to slower first shift reaction times. Moreover, the presence of gaze should lead to stronger inferences about the correct word-object mapping, resulting in higher proportion looking to targets on test trials and a decrease in first shift reaction times on exposure trials as compared to learning without a gaze cue.  

# Conclusions

Children's language comprehension and novel word learning skills are quite remarkable. Consider that even young listeners can rapidly figure out what others are saying while also building a vocabulary that will eventually reach 50,000 to 100,000 words. And they must accomplish this despite the potential for high levels of uncertainty and despite their limited cognitive resources. How do learners process and retain such a large variety of word meanings despite these challenges? 

My dissertation work provides evidence that the processes underlying children's language comprehension and word learning are quite flexible, adapting to different contexts to maintain robust comprehension and learning. In \@ref(ch1), I showed that, despite competition for visual attention in ASL processing, both children and adult signers rapidly shifted gaze away from a social partner and towards objects before sign offset, suggesting a robust link between processing an object label and quickly shifting visual attention. In \@ref(ch2), I showed that across a diverse sample of language processing contexts adults and children would adapt their gaze patterns to seek additional visual information from social partners to support robust language comprehension. In \@ref(ch3), I provided evidence that cross-situational word learners flexibly responded to the amount of ambiguity in the input, and as social information reduced referential uncertainty, they tended to store fewer word-object links. 

My proposed study will add to this line of work by integrating my research on eye movements during familiar language processing with my work measuring how social information shapes attention and memory during statistical word learning. More broadly, this study will increase our understanding of several important issues for theories of language acquisition, including: (1) how statistical learning mechanisms operate over fundamentally social input, (2) how children use eye movements as an active information gathering process to support language learning, and (3) how children's real-time behaviors support learning that unfolds over longer timescales.

\newpage

# References 

\setlength{\parindent}{-0.3in} 
\setlength{\leftskip}{0.3in}
\noindent
