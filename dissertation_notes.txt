KM DP scratch writing:


  This study aims to answer the following research questions:
    
    How does access to social cues shape in-the-moment decisions about visual fixation? How do children balance looks to people and to objects over the course of learning a new concept? Do children use prior knowledge to select fixation behaviors that best support word learning? 
    
    The word learning context is an interesting case because learners are working towards multiple goals: comprehending speech in the moment (a dynamic intergration of linguistic and visual signal with prior knowledge) and figuring out what the new word refers to in the visual scene. Thus, eye movements during concept formation can be used to gather visual information from the speaker (e.g., eye gaze or mouth movements) or about the nonlinguistic visual world (encoding objects). How do we explain where children look as they acquire more information in-the-moment of language comprehension and as they build a learning history about the correct word-object mapping?  This question can be formalized as a sequential decision making problem where children make fixation choice based on (1) their knowledge of the target concept, (2) the value of fixating a speaker for linguistic processing, and (3) the cost of each eye movement. 
  
  Framing fixation behaviors as a goal-based decision-making problem allows us to connect to formal models of action selection developed to explain 
  
  A growing body of psychological research has used the OED framework as a metaphor for active learning. The idea is that when people make decisions, they engage in a similar process of evaluating the "usefulness" of different actions relative to their learning goals. And they select behaviors that maximize the potential for gaining information. A success of the OED account is that it can capture a wide range of information seeking behaviors, including verbal question asking [@ruggeri2015children], planning interventions in causal learning tasks [@cook2011science], and decisions about where to look during scene understanding [@najemnik2005optimal]. Figures 1 and 2 present schematic overviews of how OED principles could shape the learning process for two of these domains -- causal learning (Figure 1) and word learning (Figure 2).
  
  One compelling use case of OED metaphor as a model of human behavior comes from @nelson2005finding study of eye movements during novel concept learning. Their model combined Bayesian probabilistic learning, which represents current knowledge as a probability distribution over concepts, with an OED model that calculated the usefulness of different patterns of eye movements. Here, eye movements were modeled as a type of question-asking behavior that gathered visual information about the target concept. @nelson2005finding found that participants' eye movements aligned with predictions from the OED model. Specifically, participants changed the dynamics of eye movements depending on how well they learned the target concepts. Early in learning, when the concepts were unfamiliar, the model generated a broader, less efficient distribution of fixations to explore all candidate features that could be used to categorize the stimulus. However, after the model began to learn the target concepts, eye movement patterns shifted to become more efficient and focused on a single stimulus dimension to maximize accuracy. This shift from exploratory to efficient eye movements matched adult performance on the task, suggesting that people's behavior was sensible given the structure of the learning problem and the uncertainty in the context.
  
  The intuition is that people balance fixating a speaker and fixating objects to support concept learning. The question is whether models of Bayesian concept learning and Optimal Experiment Design (Neslon & Cottrell, 2007) provide a good explanation of children’s eye movements. How far can we get using a purely computational information seeking decision model? 
    
------------------------------------------------

kim and rehder (2009): how knowledge affects attention during category learning

	- But, contra these accounts, in Experiment 2 a preference to fixate related versus neutral features began to emerge in the second block of training; by the end of training, learners were over twice as likely to fixate related features as compared to neutral ones.

	- The desire to increase speed in addition to reducing error should lead to attention shifts in situations not involving prior knowledge, and in fact this has been found. For example, Rehder and Hoffman (2005a) found that learners first discovered a one-dimensional rule that distinguished two categories but then only restricted attention (eye movements) to that dimension several (error-free) trials later. Moreover, although this result and the present experiments show that attention can shift when	learners are receiving only positive feedback, other studies have found shifts in the absence of any feedback whatsoever. For example, Blair et al. (2009a) found that learners continued to optimize attention even after a criterion of 24 correct trials was reached and feedback stopped altogether. Of course, attentional shifts in the absence of error pose problems for all category learning models that tie attentional learning to error-driven mechanisms (e.g., Kruschke, 1992).

	-  In our experiments as well, subjects began to favor related dimensions before their last error, suggesting that doing so helped them solve the learning problem. But attention shifts can also reflect learning that has already occurred, as when less valuable sources of information are bypassed in order to respond more rapidly.

	- using eyetracking we found that (a) knowledge indeed changes what features are attended, with knowledge-relevant features being fixated more often than irrelevant ones, (b) this effect was not due to an initial bias to attend relevant dimensions but rather emerged as a result of observing category members, and (c) this effect grew even after a learning criterion was reached, that is, in the absence of error feedback. 


Yu and Smith, 2016: social attention notes
	
	head-mounted eye tracking of both infant and parent. interaction around 3 novel objects

	joint attention = > 500 ms

	parents produce 60 shifts per minute; 12 mos produce 30 shifts

	infants look at objects (62%) more than faces (13%)

	sustatined attention = looks longer than 3 s

	65.38% of SA instances occurred with an accompanying parent look and thus with JA, while the rest were without JA. 
  
The present findings may help unify these two senses of responsiveness and provide a mechanistic pathway through which long-term predictions from the quality of early interactions play out. In brief, parents who are more ‘‘tuned’’ to their children’s momentary in- terests, and who are ‘‘responsive,’’ may coordinate their visual attention with that of the infant and thereby entrain and train the child’s self-regulation of atten- tion, setting up a cascade of ‘‘down-the- road’’ effects. 


goldstein social interaction and babbling

	Does social contingency provide opportunities for vocal learning?

	 Contingent responses from adults such as touching, smiling, and shaking a rattle are effective reinforcers of infant vocalizations, leading to increased rates of production (17–20).


	 key manipulation: 
	 	Half of the mothers (CC) were told to react immediately after their infants vocalized, and half (YC) were instructed to respond by the experimenter on the basis of the response schedules generated by the CC mothers. Pairings between CC and YC mothers were random

	 	Mothers wore a wireless receiver and lightweight, open-ear headphones, allowing them to receive instructions from the exper- imenter. The headphones did not obstruct the mothers’ hearing of outside sounds, but they did keep the infants from hearing the experimenter’s com- munication with their mothers.

	 Changes in babbling were not a result of maternal modeling; the social feedback was not similar in form or even modality to infants’ changes in vocalizations.



goldstein object directed vocalization = readiness to learn

	Responding to an ODV by labeling the object posi- tively correlated with vocabulary size at 15 months. By contrast, responding to an ODV by saying a word that bore an acoustic resemblance to the bab- ble (e.g., saying ‘‘bottle’’ after the infant uttered ‘‘ba’’) but was not related to the object at hand negatively correlated with vocabulary size at 15 months (Goldstein & Schwade, 2010). 

	Thus, mothers who provided object labels to the early vocalizations of their infants may have facilitated later word learn- ing by helping their infants recognize connections between sounds and objects.

	We hypothesize that ODVs are diagnostic of infant attention

	For example, when infants direct a vocalization at a toy or a picture in a book, mothers sometimes respond by labeling the object or picture (e.g., Bruner, 1983; Newson, 1977). The response comes at a moment when the infant is focused on an object, which may facilitate learning of word–object associations.

	For example, 9-month-old infants learned to produce specific phonological patterns based on the statistical regularities in caregivers’ speech that was uttered within 2 sec of their babbling (Goldstein & Schwade, 2008). Infants did not learn new vocal forms when caregiver behavior was not contingent on their bab- bles.

	In a still face paradigm, 5-month-olds showed an extinc- tion burst (a temporary increase) in babbling when social responsiveness was terminated, indicating that they had learned that their prelinguistic vocalizations should obtain social responses

	In the second trial, infants did not show significant differences in looking at the stimuli in either condition. It is likely that infants learned about the novel objects during the first looking trial; thus, they were no longer novel by the second trial and infants no longer showed a novelty preference. Such changes in looking time over trials are typical of looking time paradigms with children in this age range (e.g., Pruden et al., 2006).

	Experiment 2 showed that labeling an object contingently on an ODV facilitated learning associations between words and objects. Yoked control infants, who received labels after an equivalent amount of exposure to the objects but not contingently on an ODV, did not learn word–object associations.

	Young infants (aged 7–8 months) can learn to associate sounds with objects when they are synchronized, such that infants dishabituate when word–object pairings are switched during a habituation task (Gogate & Bahrick, 2001). 